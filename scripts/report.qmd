---
title: "Huang, Cards 2024"

# format: 
#   docx:
#     reference-doc: "./custom-reference-doc.docx"

format:
  html:
    embed-resources: true
    toc: true

execute: 
  echo: false
---

```{python}
from redcap import Project
from great_tables import GT, md, html
from statsmodels.stats.weightstats import ttest_ind
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import sys
```

```{python}
# Import project-specific functions
# Set the path to the custom functions directory
sys.path.append("../functions")

from functions import get_api_key
```

```{python}
# Get pre data
# Project_id = 26779
api_url = 'https://redcap.ucdenver.edu/api/'
api_key = get_api_key(26779)
project = Project(api_url, api_key)
pre = project.export_records()
pre = pd.DataFrame.from_dict(pre)
pre["timepoint"] = "pre"
```

```{python}
# Get post data
# Project_id = 26791
api_url = 'https://redcap.ucdenver.edu/api/'
api_key = get_api_key(26791)
project = Project(api_url, api_key)
post = project.export_records()
post = pd.DataFrame.from_dict(post)
post["timepoint"] = "post"
```

```{python}
# Define a function that generates frequency and proportions of one column
def freq_prop(df, col_name):
    
    """ Returns a GT object with the n(%) values sorted descending.
    Keyword arguments:
    df       -- a dataframe
    col_name -- the name of the column in quotes 
    """
    # Get the total sample size from the column
    N = str(df.shape[0])

    return(
        GT(pd.DataFrame(
            (df[col_name].value_counts().astype(str)) +
            ' (' +
            (df[col_name].value_counts(normalize = True).mul(100).round(1).astype(str) + '%)'))
            .reset_index()
            .rename(columns = {0:("N = " + N)})
            )
            )
```

## Ethnicity
```{python}
# Recode numeric values to categorical
pre['Ethnicity'] = pre['ethnicity'].replace({
    '1':	'American Indian / Alaskan Native',
    '2':	'Asian',
    '3':	'Black or African American',
    '4':	'Hispanic, Latinx or Spanish',
    '5':	'Middle Eastern / Northern African',
    '6':	'Pacific islander / Native Hawaiian',
    '7':	'White',
    '8':	'Prefer not to answer',
    '9':	'Other'})

# Create the basic table
freq_prop(pre, "Ethnicity")
```

## Role
```{python}
pre['Role'] = pre['role'].replace({
    '1':	'Student',
    '2':	'Resident',
    '3':	'Attending Physician',
    '4':	'Advanced Practice Provider',
    '5':	'Clinical Staff',
    '6':	'Other'})

# Create the basic table
freq_prop(pre, "Role")
```

## Gender
Categories with fewer than 5 responses were collapased into "Prefer not to answer/Other" to preserve anonymity.
```{python}
pre['Gender'] = pre['gender'].replace({
    '1':	'Female',
    '2':	'Male',
    '3':	'Prefer not to answer/Other',
    '4':	'Prefer not to answer/Other',
    '5':	'Prefer not to answer/Other',
    '6':	'Prefer not to answer/Other',
    '7':    'Prefer not to answer/Other'})

# Create the basic table
freq_prop(pre, "Gender")
```

```{python}
# Define a function to stack data, coalese, and rename for plotting
def plot(col_1, col_2):
    # Stack the two data frames
    df = pd.concat([pre[[col_1, "timepoint"]], 
                    post[[col_2, "timepoint"]]])
    
    # Remove nulls
    df = df[df[col_1] != '']
    df = df[df[col_2] != '']

    # Coalese the dependent variable into one column
    df["value"] = df[col_1].combine_first(df[col_2]).astype(int)

    ax = sns.barplot(df, x = "timepoint", y="value", alpha=.8)
    ax.set(xlabel = "Timepoint", ylabel = "Mean Response")
    ax.set(ylim=(1, 4))
    sns.despine(bottom = False, left = False, top = True, right = True)
    plt.show()
```

# Analyses
```{python}
#| eval: true
# Prepare a data frame for q1 and q2 
df = pd.concat(
    [pd.concat([
        pre[["timepoint", "witness_microaggression"]].rename(columns = {"witness_microaggression": "q1"}),
        post[["timepoint", "confident_intervene"]].rename(columns = {"confident_intervene": "q1"})]),
    pd.concat([
        pre[["likely_to_witness"]].rename(columns = {"likely_to_witness": "q2"}),
        post[["likely_intervene"]].rename(columns = {"likely_intervene": "q2"})])],
    axis = 1
)

# For q1
dict = {'1': 'Not at all confident',
    '2': 'Slightly confident',
    '3': 'Moderately confident',
    '4': 'Very confident',
    '': None}

df["q1"] = df["q1"].replace(dict)

# For q2
dict = {'1': 'Not at all likely',
    '2': 'Slightly likely',
    '3': 'Moderately likely',
    '4': 'Very likely',
    '': None}

df["q2"] = df["q2"].replace(dict)

df = df.melt(id_vars = "timepoint",
        value_vars = ["q1", "q2"],
        var_name = "question",
        value_name = "response"
        )

# Drop any missing values
df = df.dropna()

# Set a categorical
df["timepoint"] = pd.Categorical(df.timepoint,
    categories = ["pre", "post"],
    ordered = True)

df["response"] = pd.Categorical(
    df.response,
    categories = ['Not at all likely',
                  'Slightly likely',
                  'Moderately likely',
                  'Very likely',
                  'Not at all confident',
                  'Slightly confident',
                  'Moderately confident',
                  'Very confident'],
    ordered = True)
```

```{python}
# Define a function to perform independent t-tests from the pre and post dataframes
def test(col_1, col_2, label):
    """ Returns a table with the n, mean, std, of two variables and an 
    independent samples t-test
    
    Keyword arguments:
    col_1   -- pandas series of values in sample of group 1
    col_2   -- pandas series of values in sample of group 2
    label   -- character string of  the label to display in 
               the table
    """
    # Filter out any non numerical values
    x1 = pre[col_1][pre[col_1] != ''].astype(int).to_numpy()
    x2 = post[col_2][post[col_2] != ''].astype(int).to_numpy()
    
    # Collect results from t-test into a data frame
    result = ttest_ind(
        x2,
        x1,
        alternative="two-sided",
        usevar = "pooled",
        weights = (None, None),
        value = 0
        )

    # Return a data frame assembled from results
    return(
        pd.DataFrame.from_dict(
            {
            "Question":     [label],
            "N Pre":        [np.count_nonzero(x1)],
            "Mean Pre":     [x1.mean().round(2)],
            "Std Pre":      [x1.std().round(2)],
            "N Post":       [np.count_nonzero(x2)], 
            "Mean Post":    [x2.mean().round(2)],
            "Std Post":     [x2.std().round(2)],
            "Tstat":        [result[0].round(2)],
            "Pval":         [result[1].round(2)],
            "df":           [result[2]],
            }
        ).set_index("Question")
    )
```

```{python}
# Define a function that generates frequency and proportions of one column
def pre_post_tab(question):
    """ Returns a pandas crosstab in proportions of the response of a 
    categorical variable by timepoint. 
    
    Requires a prepped data frame called df specifically crafted for the
    Huang, Cards 2024 survey 
    Keyword arguments:
    question -- a character string specifying either 'q1' or 'q2'
    """
    temp = df[df['question'] == question]
    return(
    (pd.crosstab(
        temp.response,
        temp.timepoint,
        normalize = 'index'
        ) * 100).round(2).astype(str) + "%"
    )
```

## Confidence in ability to intervene
- Pre: How confident are you in your ability to intervene when witnessing microaggressions or bias?
- Post: After completing the Cards for Humanity activity, how confident are you in your ability to intervene when witnessing microaggressions or bias?
```{python}
#| eval: false
pre_post_tab('q1')
```

```{python}
# Display the scoring system
GT(pd.DataFrame({"Value": [1, 2, 3, 4],
              "Response": ["Not at all confident",
                           "Slightly confident",
                           "Moderately confident",
                           "Very confident"]})
            .set_index("Value")
            .reset_index()
)
```

```{python}
test("witness_microaggression", 
     "confident_intervene",
     "Confidence in ability to intervene")
```

```{python}
plot("witness_microaggression", "confident_intervene")
```

## Likeliness of intervening
- Pre: How likely are you to intervene when you witness microaggressions or bias?
- Post: After completing the Cards for Humanity activity, how likely are you to intervene when you witness microaggressions or bias?
```{python}
#| eval: false
pre_post_tab('q2')
```

```{python}
# Display the response key
GT(pd.DataFrame({"Value": [1, 2, 3, 4],
              "Response": ["Not at all likele",
                           "Slightly likely",
                           "Moderately likely",
                           "Very likely"]})
            .set_index("Value")
            .reset_index()
)
```

```{python}
# table 
test("likely_to_witness", 
     "likely_intervene", 
     "How likely to intervene")
```

```{python}
plot("likely_to_witness", "likely_intervene")
```

## Effectiveness of cards activity
How effective was the Cards for Humanity activity in giving you tools to become an upstander?

```{python}
dict = {'1': 'Not at all effective',
    '2': 'Slightly effective',
    '3': 'Moderately effective',
    '4': 'Very effective',
    '': None}

post['Response'] = post['effective_post'].replace(dict)

# Create the basic table
freq_prop(post, "Response")
```

```{python}
#| eval: false
# Display a table of the available responses
GT(pd.DataFrame({"Value": [1, 2, 3, 4],
              "Response": ["Not at all effective",
                           "Slightly effective",
                           "Moderately effective",
                           "Very effective"]})
            .set_index("Value")
            .reset_index()
)
```

```{python}
#| eval: false
# Display a table with the N, mean, and std
summary = pd.DataFrame(
    {
        "N":       [post["effective_post"].astype(int).count()],
        "Mean":    [post["effective_post"].astype(int).mean().round(2)],
        "Std":     [post["effective_post"].astype(int).std().round(2)]
    }
)

blank_index=[''] * len(summary)
summary.index=blank_index
summary
```

```{python}
# Clean up some of the free text responses
post['Free_text_responses'] = post['commitment'].replace({'n/a':    [None],
              'N/a':    [None],
              '     ':  [None],
              'Abdhdg': [None],
              '':       [None],
              'NA':     [None],
              'None':   [None],
              'N/A':    [None],
              'No':     [None],
              'no':     [None],
              'Na':     [None],
              '.':      [None],
              'none':   [None],
              'NA ':    [None],
              '. ':     [None],
              "-":      [None],
              ' ':      [None],
              ', ':     [None]
              })
              
 
# Print free responses to a .csv file
(pd.DataFrame(post['Free_text_responses']
.unique())
.to_csv("../data/free_text_responses.csv", index = False)
)
```

```{python}
#| eval: false
# Chi square test
# Pre vs post
from scipy.stats import chi2_contingency

dict = {'1': 'Not at all likely',
    '2': 'Slightly likely',
    '3': 'Moderately likely',
    '4': 'Very likely',
    '': None}

pre["Q1_pre"] = pre["likely_to_witness"].replace(dict)

post["Q1_post"] = post["likely_intervene"].replace(dict)

data = [[pre["Q1_pre"].dropna().value_counts().sort_index().to_numpy()],
        [post["Q1_post"].dropna().value_counts().sort_index().to_numpy()]]

data = [[90, 15, 99, 13], [112, 2, 20, 60]]

# The values in data can be a list or an array
# output the test stat, p, dof, expected
stat, p, dof, expected = chi2_contingency(data)
```